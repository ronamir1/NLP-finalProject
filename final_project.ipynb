{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pywikibot\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "catalog_df = pd.read_csv(\"public_domain_dump-master/pseudocatalogue.csv\")\n",
    "i = 6\n",
    "i"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ONLY_WORDS_AND_DIGITS_REGEX = re.compile(r'[\\w\\dא-ת]+')\n",
    "\n",
    "def get_only_words_and_digits(text: str):\n",
    "    return ONLY_WORDS_AND_DIGITS_REGEX.findall(text)\n",
    "\n",
    "def invert_words(words: list):\n",
    "    return [w[::-1] for w in words]\n",
    "\n",
    "def remove_last_line_from_string(s):\n",
    "    s = s.split(\"את הטקסט לעיל הפיקו מתנדבי פרויקט בן־יהודה באינטרנט.  הוא זמין תמיד בכתובת הבאה\")\n",
    "    return s[0]\n",
    "\n",
    "\n",
    "def remove_author_title(s, author, title):\n",
    "    s = s.replace(title, \"\", 1)\n",
    "    return s.replace(author, \"\")\n",
    "\n",
    "def normalize_text(s):\n",
    "    s = re.sub(r'[\\u0591-\\u05BD\\u05BF-\\u05C2\\u05C4-\\u05C7]', '', s)\n",
    "    return s.strip()\n",
    "\n",
    "def parse_place(place):\n",
    "    place = place.replace(\"\\n\", \"$\")\n",
    "    place = place.split(\"$$$$$$$$$$$$$$$$\")[1]\n",
    "    return place.split(\"$$$$$$$$$\")[0]\n",
    "\n",
    "def parse_date(date):\n",
    "    date = date.replace(\"\\n\", \"$\")\n",
    "    date = date.split(\"$$$$$$$$$$$$$$$$\")[1]\n",
    "    date = date.split(\"$$$$$$$$$\")[0]\n",
    "    date = date.split(\"Gregorian\")[0]\n",
    "\n",
    "    if len(date) == 4:\n",
    "        return datetime.datetime.strptime(date, '%Y')\n",
    "    return  datetime.datetime.strptime(date, '%d %B %Y')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 0\n",
    "content = []\n",
    "for index, row in catalog_df.iterrows():\n",
    "    print(index)\n",
    "    path = row[\"path\"]\n",
    "    title = normalize_text(row[\"title\"])\n",
    "    author = row[\"authors\"]\n",
    "    path = \"public_domain_dump-master/txt_stripped\" + path + \".txt\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "        text = remove_last_line_from_string(text)\n",
    "        text = remove_author_title(text, author, title)\n",
    "        text = get_only_words_and_digits(text)\n",
    "        content.append(text)\n",
    "    i += 1\n",
    "catalog_df[\"text\"] = content"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_writer(writer, id):\n",
    "    site = pywikibot.Site(\"he\", \"wikipedia\")\n",
    "    page = pywikibot.Page(site, writer)\n",
    "    try:\n",
    "        item = pywikibot.ItemPage.fromPage(page)\n",
    "        page_id = item.id\n",
    "        URL = f\"https://www.wikidata.org/wiki/{page_id}\"\n",
    "    except:\n",
    "\n",
    "        URL = f\"https://he.wikipedia.org/?curid={page.pageid}\"\n",
    "        response = requests.get(URL)\n",
    "        soup = BeautifulSoup(response.text, 'html')\n",
    "        URL = soup.find('a', {'title' : 'קישור לפריט המשויך במאגר הנתונים [g]'})['href']\n",
    "\n",
    "\n",
    "    response = requests.get(URL)\n",
    "    print(response, writer)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html')\n",
    "\n",
    "\n",
    "    try:\n",
    "        sex = soup.find('a', {'title' : 'Q6581097'}).string\n",
    "    except:\n",
    "        sex = \"female\"\n",
    "\n",
    "\n",
    "    place_of_birth = soup.find(\"div\", {\"id\": \"P19\"}).text\n",
    "    try:\n",
    "        place_of_birth = parse_place(place_of_birth)\n",
    "    except:\n",
    "        print(\"p birth - \", sys.exc_info())\n",
    "\n",
    "    place_of_death = soup.find(\"div\", {\"id\": \"P20\"}).text\n",
    "    try:\n",
    "        place_of_death = parse_place(place_of_death)\n",
    "    except:\n",
    "        print(\"p death - \", sys.exc_info())\n",
    "\n",
    "\n",
    "    date_of_birth = soup.find(\"div\", {\"id\": \"P569\"}).text\n",
    "    try:\n",
    "        date_of_birth = parse_date(date_of_birth)\n",
    "    except:\n",
    "        print(\"d birth - \", sys.exc_info())\n",
    "\n",
    "    date_of_death = soup.find(\"div\", {\"id\": \"P570\"}).text\n",
    "    try:\n",
    "        date_of_death = parse_date(date_of_death)\n",
    "    except:\n",
    "        print(\"d death - \", sys.exc_info())\n",
    "\n",
    "    return [id, writer, sex, place_of_birth, place_of_death, date_of_birth, date_of_death]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "writers = catalog_df[\"authors\"].unique()\n",
    "authors_df = pd.DataFrame(columns=[\"id\", \"name\", \"sex\", \"p_birth\", \"p_death\", \"d_birth\", \"d_death\"], )\n",
    "for i, writer in enumerate(writers):\n",
    "    try:\n",
    "        authors_df.loc[i] = parse_writer(writer, i)\n",
    "    except Exception:\n",
    "        authors_df.loc[i] = [i, writer, None, None, None, None, None]\n",
    "        print(sys.exc_info())\n",
    "        print(\"couldn't complete for\", writer, i)\n",
    "print(authors_df)\n",
    "authors_df.to_csv(\"authors\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "\n",
    "def get_year(year_str):\n",
    "    try:\n",
    "        if year_str and year_str is not np.nan:\n",
    "            if '0000' in year_str:\n",
    "                return 0\n",
    "            return parser.parse(year_str).year\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def parse_authors(df, cities):\n",
    "    data = []\n",
    "    for index, row in df.iterrows():\n",
    "        id = row[\"id\"]\n",
    "        male = row[\"sex\"] == \"male\"\n",
    "        female = row[\"sex\"] == \"female\"\n",
    "        birth_year = get_year(row[\"d_birth\"])\n",
    "        death_year = get_year(row[\"d_death\"])\n",
    "        if birth_year is None or death_year is None:\n",
    "            continue\n",
    "        b_ancient = birth_year <=500\n",
    "        b_spain = 500 < birth_year <= 1400\n",
    "        b_renaissance = 1400 < birth_year <= 1800\n",
    "        b_19ct = 1800 < birth_year <= 1900\n",
    "        b_20ct = 1900 < birth_year <= 2000\n",
    "        b_modern = 2000 < birth_year\n",
    "        d_ancient = death_year <=500\n",
    "        d_spain = 500 < death_year <= 1400\n",
    "        d_renaissance = 1400 < death_year <= 1800\n",
    "        d_19ct = 1800 < death_year <= 1900\n",
    "        d_20ct = 1900 < death_year <= 2000\n",
    "        d_modern = 2000 < death_year\n",
    "        hebrew_speaker = \"FALSE\" != row[\"Hebrew Speaker\"]\n",
    "        p_birth_israel = row[\"p_birth\"] in cities\n",
    "        p_birth_not_israel = row[\"p_birth\"] not in cities\n",
    "        p_death_israel = row[\"p_death\"] in cities\n",
    "        p_death_not_israel = row[\"p_death\"] not in cities\n",
    "        data.append([id, male, female, b_ancient, b_spain, b_renaissance, b_19ct,\n",
    "                     b_20ct, b_modern, d_ancient, d_spain, d_renaissance, d_19ct,\n",
    "                     d_20ct, d_modern, hebrew_speaker, p_birth_israel, p_birth_not_israel,\n",
    "                     p_death_israel, p_death_not_israel])\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "authors_df = pd.read_csv('authors.csv')\n",
    "israeli_cities = [\"Tel Aviv\", \"Jaffa\", \"Jerusalem\", \"Haifa\", \"Nahalal\", \"Acre\", \"Ramat Gan\", \"Givat Hashlosha\",\n",
    "                  \"Rishon LeZion\", \"Petah Tikva\", \"Zikhron Ya'akov\", \"Israel\", \"Mandatory Palestine\"]\n",
    "parsed_authors_data = parse_authors(authors_df, israeli_cities)\n",
    "parsed_authors_df = pd.DataFrame(parsed_authors_data, columns=[\"id\", \"male\", \"female\", \"b_ancient\", \"b_spain\", \"b_renaissance\", \"b_19ct\",\n",
    "                     \"b_20ct\", \"b_modern\", \"d_ancient\", \"d_spain\", \"d_renaissance\", \"d_19ct\",\n",
    "                     \"d_20ct\", \"d_modern\", \"hebrew_speaker\", \"p_birth_israel\", \"p_birth_not_israel\",\n",
    "                     \"p_death_israel\", \"p_death_not_israel\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "parsed_authors_df.to_csv(\"authors_parsed.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "hebnlp",
   "language": "python",
   "display_name": "hebnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}